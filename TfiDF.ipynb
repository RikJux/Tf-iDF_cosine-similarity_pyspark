{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5872ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f74c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0', \" If you don't do stupid things you won't end up in tragedy.\"),\n",
       " ('1', ' Blessings come in disguise.'),\n",
       " ('2', \" If small holes aren't fixed, then big holes will bring hardship.\"),\n",
       " ('3', ' Water flows in only to flow out.'),\n",
       " ('4',\n",
       "  \" It's better to walk thousands of miles than to read thousands of books.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\"Chinese_proverbs.csv\", header=True).rdd\n",
    "data = data.map(lambda x: (x[\"ID\"], x[\"text\"]))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb82938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 10),\n",
       " (16, 2),\n",
       " (7, 4),\n",
       " (19, 1),\n",
       " (1, 368),\n",
       " (2, 72),\n",
       " (14, 1),\n",
       " (4, 16),\n",
       " (3, 26),\n",
       " (26, 1),\n",
       " (8, 2),\n",
       " (21, 1),\n",
       " (6, 6),\n",
       " (36, 1),\n",
       " (68, 1),\n",
       " (31, 1),\n",
       " (9, 4),\n",
       " (17, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# histogram of frequencies of terms\n",
    "#return tuples (frequency, number of terms that have that frequency)\n",
    "from string import punctuation\n",
    "\n",
    "def splitInTerms(x):\n",
    "    doc_id, text = x\n",
    "    text = text.translate(str.maketrans(punctuation, ' '*len(punctuation)))\n",
    "    text = text.split()\n",
    "    res = []\n",
    "    for t in text:\n",
    "        res.append((t.lower(), +1))\n",
    "        \n",
    "    return res\n",
    "\n",
    "data.flatMap(lambda x: splitInTerms(x))\\\n",
    "    .reduceByKey(lambda l,r : l+r)\\\n",
    "    .map(lambda x: (x[1], +1))\\\n",
    "    .reduceByKey(lambda l,r : l+r)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0856fea",
   "metadata": {},
   "source": [
    "#### TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe0679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('if', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('you', {'doc': '0', 'TF': 0.14285714285714285}),\n",
       " ('don', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('t', {'doc': '0', 'TF': 0.14285714285714285}),\n",
       " ('do', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('stupid', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('things', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('won', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('end', {'doc': '0', 'TF': 0.07142857142857142}),\n",
       " ('up', {'doc': '0', 'TF': 0.07142857142857142})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each term in the document I create a new tuple (term, doc_id, +1)\n",
    "# and then distribute the count of occurrences of a term in a specific document\n",
    "\n",
    "def splitInTerms(x):\n",
    "    doc_id, text = x\n",
    "    text = text.translate(str.maketrans(punctuation, ' '*len(punctuation)))\n",
    "    text = text.split()\n",
    "    res = []\n",
    "    for t in text:\n",
    "        res.append(((t.lower(), doc_id, len(text)), +1))\n",
    "        \n",
    "    return res\n",
    "\n",
    "data_TF = data.flatMap(lambda x: splitInTerms(x))\\\n",
    "    .reduceByKey(lambda l, r: l+r)\\\n",
    "    .map(lambda x: (x[0][0], {\"doc\":x[0][1], \"TF\":x[1]/x[0][2]}))\n",
    "\n",
    "data_TF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ec9cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('if', 3.2347491740244907),\n",
       " ('you', 2.5416019934645457),\n",
       " ('don', 3.0524276172305362),\n",
       " ('t', 2.136136885356381),\n",
       " ('do', 4.844187086458591),\n",
       " ('stupid', 4.844187086458591),\n",
       " ('things', 4.151039905898646),\n",
       " ('won', 4.844187086458591),\n",
       " ('end', 4.151039905898646),\n",
       " ('up', 4.844187086458591)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import log\n",
    "numelem = data.count()\n",
    "\n",
    "data_iDF = data_TF.map(lambda x: (x[0], +1))\\\n",
    "    .reduceByKey(lambda l,r: l+r)\\\n",
    "    .map(lambda x: (x[0], log(numelem/x[1])))\n",
    "data_iDF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfd9946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', {'term': 'don', 'TF-iDF': 0.21803054408789543}),\n",
       " ('25', {'term': 'don', 'TF-iDF': 0.38155345215381703}),\n",
       " ('35', {'term': 'don', 'TF-iDF': 0.3391586241367262}),\n",
       " ('54', {'term': 'don', 'TF-iDF': 0.3591091314388866}),\n",
       " ('57', {'term': 'don', 'TF-iDF': 0.30524276172305365}),\n",
       " ('112', {'term': 'don', 'TF-iDF': 0.6104855234461073}),\n",
       " ('0', {'term': 'do', 'TF-iDF': 0.34601336331847077}),\n",
       " ('0', {'term': 'stupid', 'TF-iDF': 0.34601336331847077}),\n",
       " ('0', {'term': 'things', 'TF-iDF': 0.29650285042133184}),\n",
       " ('59', {'term': 'things', 'TF-iDF': 0.5188799882373307})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put together TF and iDF\n",
    "\n",
    "data_TF_iDF = data_TF.join(data_iDF)\\\n",
    "    .map(lambda x: (x[1][0][\"doc\"], {\"term\":x[0], \"TF-iDF\":(x[1][1]*x[1][0][\"TF\"])}))\\\n",
    "    .filter(lambda x: x[1][\"TF-iDF\"] != 0)\n",
    "\n",
    "data_TF_iDF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac80dfd",
   "metadata": {},
   "source": [
    "#### cosine similarity\n",
    "La formula di cosine similarity Ã¨ composta da tre termini: uno a numeratore e due a denominatore. Ciascuno dei due termini a denominatore si riferisce ad un unico documento. Ho scelto prima di calcolare per ogni documento il suo termine denominatore, poi creare un dizionario distribuito {doc:'', den:''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb165e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate denominator for each document\n",
    "\n",
    "df_denom = data_TF_iDF.map(lambda x: (x[0], x[1][\"TF-iDF\"]**2))\\\n",
    "    .reduceByKey(lambda l,r: l+r)\\\n",
    "    .map(lambda x: (x[0], x[1]**(-1/2)))\n",
    "\n",
    "denoMap = sc.broadcast(df_denom.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa524773",
   "metadata": {},
   "source": [
    "Occorre calcolare il termine a numeratore per ogni coppia di documenti. Per calcolarlo bisogna moltiplicare i punteggi TF-iDF di ogni termine in comune e poi calcolare la sommatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9459f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('0', '10'), 0.039758601277546955),\n",
       " (('1', '10'), 0.13915510447141435),\n",
       " (('3', '32'), 0.13252867092515652),\n",
       " (('3', '42'), 0.19879300638773478),\n",
       " (('3', '43'), 0.08835244728343766),\n",
       " (('3', '67'), 0.100826890659524),\n",
       " (('10', '14'), 0.1346981234449684),\n",
       " (('32', '42'), 0.2319251741190239),\n",
       " (('32', '43'), 0.10307785516401062),\n",
       " (('32', '67'), 0.07136159203662275)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate numerator for each pair of documents\n",
    "df_num = data_TF_iDF.map(lambda x: (x[1][\"term\"], {\"doc\":x[0], \"TF-iDF\":x[1][\"TF-iDF\"]}))\n",
    "df_num = df_num.join(df_num)\\\n",
    "    .filter(lambda x: x[1][0][\"doc\"] < x[1][1][\"doc\"])\\\n",
    "    .map(lambda x: ((x[1][0][\"doc\"], x[1][1][\"doc\"]), x[1][0][\"TF-iDF\"] * x[1][1][\"TF-iDF\"]))\\\n",
    "    .reduceByKey(lambda l,r: l+r)\n",
    "\n",
    "df_num.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67889a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine = df_num.map(lambda x: (x[0], x[1]*(denoMap.value.get(x[0][0])*denoMap.value.get(x[0][1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d53bb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10',\n",
       "  [('36', 0.1802748783626977),\n",
       "   ('14', 0.10346474824158061),\n",
       "   ('42', 0.10222955623926848),\n",
       "   ('16', 0.09137078125171756),\n",
       "   ('100', 0.07708970671626461)]),\n",
       " ('26',\n",
       "  [('69', 0.21184053001829448),\n",
       "   ('54', 0.17316665285465507),\n",
       "   ('49', 0.16804707356284873),\n",
       "   ('14', 0.1443547248544675),\n",
       "   ('3', 0.13655043369226058)]),\n",
       " ('4',\n",
       "  [('27', 0.2271708145975577),\n",
       "   ('8', 0.12776085947663285),\n",
       "   ('26', 0.12298829180489189),\n",
       "   ('22', 0.12047882762059949),\n",
       "   ('66', 0.1109930978462955)]),\n",
       " ('102',\n",
       "  [('49', 0.15615371503142822),\n",
       "   ('74', 0.10728452585597198),\n",
       "   ('118', 0.09847890104766069),\n",
       "   ('113', 0.07971450269718441),\n",
       "   ('122', 0.07966025181512619)]),\n",
       " ('113',\n",
       "  [('54', 0.16416265047281103),\n",
       "   ('17', 0.14837381152994536),\n",
       "   ('95', 0.12435533752771984),\n",
       "   ('97', 0.12394984397667404),\n",
       "   ('118', 0.11567813316391383)]),\n",
       " ('20',\n",
       "  [('58', 0.32799135387918693),\n",
       "   ('42', 0.2384297372667188),\n",
       "   ('55', 0.21536551227772616),\n",
       "   ('97', 0.15520335171027866),\n",
       "   ('80', 0.1425197102522829)]),\n",
       " ('54',\n",
       "  [('0', 0.32755455213388274),\n",
       "   ('112', 0.21360785730534432),\n",
       "   ('57', 0.19841885154145888),\n",
       "   ('35', 0.18861911132488035),\n",
       "   ('24', 0.17533203485267249)]),\n",
       " ('60',\n",
       "  [('97', 0.15925501304948544),\n",
       "   ('95', 0.1004938271619797),\n",
       "   ('80', 0.09673424501018052),\n",
       "   ('4', 0.07862671997169116),\n",
       "   ('54', 0.07849627504066427)]),\n",
       " ('88',\n",
       "  [('121', 0.13545204739092195),\n",
       "   ('85', 0.13242953934589669),\n",
       "   ('49', 0.11457969519722977),\n",
       "   ('4', 0.08707195933909928),\n",
       "   ('26', 0.08031296524165714)]),\n",
       " ('108',\n",
       "  [('121', 0.25321401379722813),\n",
       "   ('79', 0.16447054847970977),\n",
       "   ('104', 0.1121029307386197),\n",
       "   ('80', 0.07770644134492836),\n",
       "   ('41', 0.07694663326981548)])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each document return the five most similar to it\n",
    "def get_best_n(l, r, n=5):\n",
    "    x = sorted(l+r, key=lambda t: t[1], reverse=True)[0:n]\n",
    "    return x\n",
    "\n",
    "df_cosine.flatMap(lambda x: [(x[0][0], [(x[0][1], x[1])]), (x[0][1], [(x[0][0], x[1])])])\\\n",
    "    .reduceByKey(lambda l, r: get_best_n(l,r))\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b8917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
